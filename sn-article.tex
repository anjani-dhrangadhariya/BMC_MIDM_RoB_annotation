% Version 1.2 of SN LaTeX, November 2022
%
% See section 11 of the User Manual for version history 
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst, sn-mathphys.bst. %  
 
%%\documentclass[sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[sn-mathphys,Numbered]{sn-jnl}% Math and Physical Sciences Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style 
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style
%%\documentclass[default]{sn-jnl}% Default
%%\documentclass[default,iicol]{sn-jnl}% Default with double column layout

%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
%%%%
%\usepackage{caption}

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%\jyear{2021}%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

%\title[Article Title]{Developing Comprehensive Annotation Guidelines and a Corpus of Risk of Bias Assessment for Rehabilitation: A Methodological Approach}
\title[Article Title]{RoBuster: A Validation Corpus with Risk of Bias span annotations in Randomized Controlled Trials}

%%=============================================================%%
%% Prefix	-> \pfx{Dr}
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% NatureName	-> \tanm{Poet Laureate} -> Title after name
%% Degrees	-> \dgr{MSc, PhD}
%% \author*[1,2]{\pfx{Dr} \fnm{Joergen W.} \spfx{van der} \sur{Ploeg} \sfx{IV} \tanm{Poet Laureate} 
%%                 \dgr{MSc, PhD}}\email{iauthor@gmail.com}
%%=============================================================%%

\author*[1,2]{\fnm{Anjani} \sur{Dhrangadhariya}}\email{anjani.dhrangadhariya@hevs.ch}

\author[3]{\fnm{Roger} \sur{Hilfiker}}\email{roger.hilfiker@proton.me}

\author[4]{\fnm{Martin} \sur{Sattelmayer}}\email{martin.sattelmayer@hevs.ch}

\author[5,6]{\fnm{Nona} \sur{Naderi}}\email{nona.naderi@hesge.ch}

\author[4]{\fnm{Katia} \sur{Giacomino}}\email{katia.giacomino@hevs.ch}
\equalcont{These authors contributed equally to this work.}

\author[4]{\fnm{Rahel} \sur{Caliesch}}\email{rahel.caliesch@hevs.ch}
\equalcont{These authors contributed equally to this work.}

\author[1]{\fnm{Stéphane} \sur{Marchand-Maillet}}\email{stephane.marchand-maillet@unige.ch}

\author[1,2]{\fnm{Henning} \sur{Müller}}\email{henning.mueller@hevs.ch}


\affil*[1]{\orgdiv{Department of Computer Science}, \orgname{University of Geneva}, \orgaddress{\city{Geneva}, \country{Switzerland}}}

\affil[2]{\orgdiv{Informatics Institute}, \orgname{HES-SO Valais-Wallis}, \orgaddress{\city{Sierre}, \country{Switzerland}}}

\affil[3]{\orgdiv{IUFRS}, \orgname{University of Lausanne}, \orgaddress{\city{Lausanne}, \country{Switzerland}}}

\affil[4]{\orgdiv{School of Health Sciences}, \orgname{HES-SO Valais-Wallis}, \orgaddress{\city{Leukerbad}, \country{Switzerland}}}

\affil[5]{\orgdiv{Geneva School of Business Administration}, \orgname{HES-SO Geneva}, \orgaddress{\city{Geneva}, \country{Switzerland}}}

\affil[6]{\orgname{Swiss Institute of Bioinformatics (SIB)}, \orgaddress{\city{Geneva}, \country{Switzerland}}}

%%==================================%%
%% sample for unstructured abstract %%
%%==================================%%
% Do not go beyond 200 words for the abstract.
%\abstract{The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. Authors are advised to check the author instructions for the journal they are submitting to for word limits and if structural elements like subheadings, citations, or equations are permitted.}

%%================================%%
%% Sample for structured abstract %%
%%================================%%

\abstract{\textbf{Purpose:} Risk of bias (RoB) assessment of randomized clinical trials (RCTs) is vital to answer a systematic review question accurately.
Manual RoB assessment for hundreds of clinical trials is a cognitively demanding and lengthy process.
LLMs (large language models) are immensely capable but necessitate evaluation in RoB assessment.
Currently, no RoB annotation guidelines or annotated corpus exist for LLM evaluation.
% 
\textbf{Methods:} Taking the revised Cochrane risk of bias assessment guidelines for RCTs, we develop a set of visual instructional placards that could be used as text annotation guidelines to annotate RoB spans in clinical trials.
Expert annotators employed these visual guidelines to annotate a corpus of 60 full-text RCTs from physiotherapy and rehabilitation.
We report inter-annotator agreement (IAA) among two expert annotators when applying the guidelines to a portion (9 of 60) of the annotated corpus.
The annotated corpus was used to evaluate ChatGPT using a straightforward evaluation framework.
% 
\textbf{Results:} We present a corpus of 60 RCTs with fine-grained span annotations comprising more than 11,500 tokens belonging to 22 RoB classes.
The IAA ranges between 0\% and 99\%.
LLMs show promising but variable agreements across the different bias questions.
% 
\textbf{Conclusion:} 
Despite having comprehensive bias assessment guidelines, RoB annotation remains a complex text annotation task.
Utilizing visual placards for bias assessment and annotation enhances inter-annotator agreement (IAA) compared to case where visual placards are absent.
While LLMs demonstrate potency, their accuracy falters with more subjective RoB categories.
Lastly, we address the study's strengths and limitations, offering insights for future research in the field.}

\keywords{risk of bias, dataset, natural language processing, large language models}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle


%%%%%%%%%%%%%%%%
%% Background %%
\section{Background and Significance}
\label{sec:background}
%
Systematic reviews (SRs) synthesized using randomized controlled trials (RCTs) are the highest quality of evidence in the evidence pyramid.
SRs aid medical professionals make educated decisions about an individual's health and help governments enact informed health policies~\cite{mogo2022systematic,mctigue2006obesity}.
An RCT is a scientific experiment aiming to evaluate the efficacy of an intervention on particular patient outcomes.
In these trials, patients are randomly divided and allocated to either an active intervention group or a comparator group, and the impact of intervention compared to the comparator is measured in a controlled setting~\cite{sibbald1998understanding}.
Theoretically, RCTs are low on biases given the randomized study design but are still prone to unavoidable biases creeping into the trial's design, execution, or reporting.
Biased clinical trials make medical practitioners systematically overestimate or underestimate the intervention effect on patient outcomes, leading to harmful health practices and policies~\cite{kjaergard1999randomized,naci2019design}.
Thus, reviewers conducting SRs must thoroughly screen RCTs for biases before inclusion in writing SRs.


The biases in RCTs cannot be quantified, but an RCT can be assessed for biases to minimize the overall risk and judge its quality.
In this study, we refer to bias assessment as risk-of-bias (RoB) assessment.
There are several tools to assess RoB, including the Cochrane Collaborations RoB Tool, Physiotherapy Evidence Database (PEDro) RoB scale, revised Cochrane RoB 2 tool (RoB 2), AMSTAR/AMSTAR 2, EPOC RoB Tool and several other independent checklists~\cite{higgins2011cochrane,elkins2013growth,sterne2019rob,shea2017amstar,farrah2019risk}.
These tools are structured as a series of questions aiming to elicit factual information from the RCTs, which could then be used for RoB assessment.
%These tools are a series of questions aiming to elicit factual information from the RCTs, which can then be used to assess their quality.
Manual quality assessment requires the reviewers to go through full-text RCTs and manually inspect every question from the chosen bias assessment tool.
The process takes about 3-10 months per person per SR and requires a high degree of methodological expertise on the reviewer's part.
Moreover, RoB assessment is a part of writing systematic reviews, which is highly resource-heavy, taking about six months to several years to complete the review~\cite{tsertsvadze2015conduct,khangura2012evidence,higgins2019cochrane}.
The pace at which RCTs are published makes RoB assessment a lengthy process and underscores the need for automation.



Machine learning (ML) can help accelerate the assessment process by directly pointing the reviewers to the parts of the RCT text relevant to identifying bias, leading to quickly judging the trial quality.
Marshall \textit{et al.}~\cite{marshall2015automating} attempted automation of RoB assessment using distant supervision approach supported by proprietary data from the Cochrane Database of Systematic Reviews (CDSR). 
They formulated the trial quality assessment as binary classification into \textit{low-risk} and \textit{unclear-risk/high-risk} quality attributes for each risk domain.
The study was supported by the manually-entered data from CDSR, which is behind a paywall and automates based on Cochrane's RoB 1.0 guidelines and not the latest RoB 2~\cite{higgins2011cochrane}.
Even though Cochrane's RoB tool (version 1) is the most frequently used to assess RCT quality, a recently revised Cochrane RoB 2 offers significant differences in comparison~\cite{ma2020methodological}.
Compared to the original RoB version released in 2008, the RoB 2 version provides a more reliable and concrete structure to the RoB evaluation by developing comprehensive guidelines that enforce consistency~\cite{higgins2011cochrane,sterne2019rob}.
A study analyzing Cochrane systematic reviews and protocols found that the use of RoB 2 increased from 0\% in 2019 to 24.1\% in 2022~\cite{martimbianco2023most}.
This indicates the importance of using an updated and standardized tool to assess bias in RCTs.

Millard \textit{et al.} attempted automating RoB assessment using supervised machine learning trained on proprietary data as well~\cite{millard2016machine}.
In fact, the research utilising this pay-walled data was used to develop RobotReviewer that has been evaluated by several studies for its human-competent performance~\cite{marshall2016robotreviewer,soboczenski2019machine,vinkers2021methodological,jardim2022automating,hirt2021agreement}.
The question, however, remains of the unavailability of a publicly-available RoB annotated corpus that hinders community efforts for automation. 
Wang \textit{et al.} recently released three RoB annotated datasets, but for preclinical animal studies with RoB assessments pertaining to animals~\cite{wang2022risk}.
A manually annotated corpus of RoB spans for human clinical trials is still necessary.
Manual RoB assessment is a complex, expert-led task laden with subjective judgements.
Systematically translating this manual process for developing an RoB annotated corpus requires a carefully designed annotation scheme and detailed annotation guidelines.
Recently Dhrangadhariya \textit{et al.} worked on a pilot study to test whether RoB 2 guidelines could be effectively utilized as guidelines to manually annotate a corpus of RCTs with RoB using a multi-level annotation scheme adapted from the same guidelines.
They conclude that the assesment guidelines cannot be used as text annotation guidelines, but neither provide any annotation guidelines from their end.
Additionally, the dataset they provide is comparatively small with 10 annotated RCTs~\cite{dhrangadhariya2023first}.
Our objective is to develop clear cut annotation guidelines to annotate RCTs with RoB spans corresponding to RoB 2 tool for randomized controlled trials~\cite{sterne2019rob}.



Recently, large language models (LLMs) have demonstrated exceptional performance on unseen tasks when only the task instructions are provided~\cite{chang2023survey}.
However, till date, no one has evaluated their performance on the cognitively complex task of identifying RoB text descriptions from RCTs and providing their RCT quality judgments based on text.
Our contributions with this paper are five-fold. 
1) We develop comprehensive annotation guidelines for annotating RCTs with risk of bias description.
2) We model these annotation guidelines in form of visual placards for ease of annotation and understanding. These placards could be used as visual RoB assessment guidelines by the trainee RoB assessors.
3) We annotate a corpus of 60 full-text RCTs with 22 risk of bias span types which could be used to fine-tune machine learning models or LLMs and could also be used as a validation benchmark.
4) We evaluate the performance of LLMs to automatically identify the answers to these signalling questions using prompt generation.
5) We make the visual annotation guidelines, the dataset and LLM prompts openly-available for the scientific community.
%
%
%
\section{Methodology}
\label{sec:methods}
%
This section describes the annotation scheme, annotation softwares and guidelines.
Since there are no annotation guidelines available for RoB span annotation task, we take pleasure in formulating them from scratch. 
We first developed a draft version of our visual annotation guidelines, doubly annotated a fraction of documents using it and used the conflicts identified during this exercise to refine the guidelines.
%
%
%
\subsection{Annotation scheme}
\label{met:annot_scheme}
%

%
%
%
\begin{figure}
    \centering
    \includegraphics[width=0.99\columnwidth]{figures/annotation_schema.pdf}
    \caption{Our annotation scheme}
    \label{fig:annotationscheme}
\end{figure}
%
%
%



Creating a new annotated corpus involves defining an annotation scheme or adopting an existing one.
To our knowledge the only available annotation scheme for RoB span annotation was presented by~\cite{dhrangadhariya2023first}.
Rather than creating a new scheme, we adapt and enhance their approach by learning from its drawbacks.
Their annotation scheme was directly adapted from the RoB 2 assessment procedure and hence it is imperative to understand the RoB 2 structure to understand the annotation scheme.
RoB 2 divides biases into five risk domains, each loosely corresponding to different parts of the trial design.
Each risk domain decomposes into several signalling questions, each aiming to prompt a relevant response to bias assessment (refer to Table~\ref{tab:robdomains}).


%
%
%
\begin{table*}
 \centering
 %\captionsetup{justification=justified}
   \caption{The table lists down the bias domains as structured in the revised Cochrane RoB assessment tool (RoB 2) and the number of signalling questions (SQ) in each domain.}\label{tab:robdomains}
    \begin{tabular}{llr}
    \toprule
     Class & Domain & SQ\\
    \midrule
    RoB 1 & biases arising from the \textbf{randomization process} &  3\\
    RoB 2 & biases due to \textbf{deviations from intended interventions} & 7\\
    RoB 3 & bias due to \textbf{missing outcome data} & 4\\
    RoB 4 & bias in the \textbf{measurement of the outcome} & 5\\
    RoB 5 & bias in the \textbf{selection of the reported result} & 3\\
    \bottomrule
    \end{tabular}
\end{table*}
%
%
%


The response options are restricted to ``Yes'', ``Probably yes'', ``No'', ``Probably no'', or ``No information''~\cite{sterne2019rob}.
Reviewers assess these signalling questions by examining the factual evidence in the RCT.
For instance, to answer the signalling question ``Was the allocation sequence random?'', the reviewer reads through the study to identify how participants were randomized into intervention groups.
If a well-executed method of randomization is identified, the reviewer answers with ``yes'' (the allocation sequence is random) judging the risk of bias for this signaling question as low risk.
Conversely, if a poorly executed method of randomization is found, the risk of bias is deemed  high risk with response option ``no''.
%Similarly, each signalling question prompts the reviewer to look for a piece(s) of factual evidence in the clinical study to respond with one of the five response options.



In RoB span annotation, we mimic this assessment process by considering evidence text spans in the RCT as the main units of annotation. 
Each span corresponds to answering a signalling question and is annotated with the most informative label. 
The label incorporates information about the signalling question number and the domain it assesses (for the above example, ``1.1'' for the first domain and first signalling question of the domain)
Additionally, the response judgement is incorporated in the label, such as ``1.1 Yes Good'' for a well-executed randomization (see Figure~\ref{fig:annotationscheme}).
Dhrangadhariya \textit{et al.} suggests collapsing the response options ``yes'' and ``probably yes'' into a single ``yes'', and ``no'' and ``probably no'' together into a single ``no'' to increase the inter-annotator agreement (IAA) without altering the final risk domain judgment~\cite{dhrangadhariya2023first}.
As shown in Figure~\ref{fig:flowchart} responding to any signalling question for the risk domain 2 as either ``Probably yes'' or ``Yes'' does not alter the final risk judgment for this domain (low, high, or some concerns).
Therefore, except for some special case signalling questions, we too collapse these response option as suggested.
In summary, the reviewer needs to label the identified text span with the RoB entity along with one of the response options.
In this regard, we have a hierarchical span annotation scheme comprising 22 entities corresponding to the 22 signalling questions, each with typically two response options and two directions (mutually exclusive).
We also remove the ``No Information'' labels since it is meant for the situations where actually no text evidence is found in the RCT to answer a SQ.
However, for selected signalling questions, ``probably yes'', ``probably no'' and ``No Information'' may still be used.
For example, consider that an RCT uses ``...random number generator and sealed envelopes for patient randomization...'', but the trial provided no information on whether the envelop was ``opaque'' or not.
In such situations, ``No Information'' label is acceptable.
%Additionally, it is based on this span, the response judgment option is decided, and therefore even the response judgement information is incorporated in the label.
%Consider the following span is found to answer the above-mentioned signalling question, ``randomized participants using random number generation...'', which is a good randomization procedure and therefore the reviewer responds with ``yes''.
%The label extends the example label to ``1.1 Yes''.
%In addition, a direction is also added to the label, for example, whether it is ``good'' or ``bad.
%In this example, a good randomization ensure low risk for this signalling question and vice versa.
%Adding direction extends the example label to ``1.1 Yes Good''.


%
%
%
\begin{figure}
    \centering
    \includegraphics[width=0.80\columnwidth]{figures/flowchart.pdf}
    \caption{Algorithm for suggested judgement of risk of bias arising from the randomization process. The figure is recreated from the revised Cochrane's risk of bias tool (RoB 2).~\cite{sterne2019rob}}
    \label{fig:flowchart}
\end{figure}
%
%
%

%
%
%
\subsection{Data collection}
\label{data}
%
%Our corpus is focused on these domains because our annotators have RoB assessment expertise in this domain.
Savović~\textit{et al.} categorized outcome measures as mortality, other objective outcome, or subjective outcome and estimated the associations of bias judgments with intervention effect estimates~\cite{savovic2018association}.
Trials assessing subjective outcomes are more prone to bias, therefore, had we used only one outcome type, we would have limited label types for different risk classes~\cite{page2016empirical}.
In context of RCTs, subjective outcomes are measurements that rely on individuals' perceptions, opinions, or feelings about their own health or well-being.
These outcomes are typically self-reported by the participants in the trial and can be influenced by factors such as  placebo effects, patient expectations, interpretation, and psychological factors.
For example, in a study on rheumatoid arthritis, subjective outcome measures included patient-reported pain ratings~\cite{vollert2020assessment}.
Objective outcomes are measurements that are independent of individual opinions or perceptions and are based on observable and measurable data.
These outcomes are typically collected by trained assessors or through laboratory tests, imaging studies, or other objective methods.
For instance, in a study on peripheral artery disease, objective outcome measures included angiography and molecular imaging to evaluate the effectiveness of cell therapy~\cite{grimaldi2016imaging}.
Mortality outcomes refer to the occurrence of death during the course of the trial.
To ensure these different outcome types are represented in the corpus, we restricted including 26, 23 and 11 RCTs addressing objective, subjective and mortality primary outcomes. %TODO: Recheck the distribution of 26, 23, and 11
The epidemiology researcher from our team created this 60 RCTs dataset from the domain of physiotherapy and rehabilitation.
%These 60 studies were selected from four journals: journal 1, journal 2, journal 3 and journal 4. 
%TODO Anjani: Justify why only these journals were considered?
PDFs of the full-text RCTs were extracted and each article was collated with its trial protocol from wherever available.
%TODO Anjani: Add if some studies were randomly added. Ask Roger for more information.
All these RCT PDFs have CC-BY-0 licences~\footnote{\url{https://creativecommons.org/share-your-work/public-domain/cc0/}}.
%We list the details of these studies in a separate supplementary material file.
%
%
%
%\begin{itemize}
%    \item 60 RCTs
%    \item Between 2001:2021
%    \item CC0, CC BY, CC BY non-commercial
%    \item Eutils search - Include search query for PubMed
%    \item Search for each year, retmax=not defined, randomly selected 20 studies from the retrieved
%    \item Take the first three studies with free-full text available.
%    \item Upload to tagtog
%\end{itemize}
%
%
%
\subsection{Expert team}
\label{experts}
%
As mentioned earlier, RoB annotation is a complex task that requires specialized expertise.
It's cognitively demanding due to the need to carefully go through the entire full-text of RCTs and identify 22 different bias categories for annotation.
This level of complexity would not be manageable for annotators without expertise in the field.
Our annotation team consisted of two researchers specializing in RoB assessment in physiotherapy and rehabilitation domains, including an epidemiology researcher and an assistant professor in physiotherapy.
With a substantial background in both physiotherapy, advanced statistical methods and experience writing systematic reviews, both experts possesses a deep understanding of the complexities involved in assessing bias risk in research studies.
Two additional physiotherapy experts, a post-doc and a senior PhD student, were a part of developing the visual annotation guidelines.


Additionally, two researchers with expertise in natural language processing (NLP) were involved, one being a computational linguistics professor and the other a PhD student in computer science.
The inclusion of NLP researchers was particularly important because the guidelines and placards they helped create will be utilized to annotate a text corpus, which will serve as a benchmark for the NLP task of RoB span extraction.
%
%
%
\subsection{Guideline development}
\label{guidelines}
%
%
%
%
\begin{figure}
    \centering
    \includegraphics[width=0.80\columnwidth]{figures/placard_3_1.pdf}
    \caption{Sample annotation instruction placard for the signalling question 3.1 designed and adapted using RoB 2 tool.}
    \label{fig:placard}
\end{figure}
%
%
%

RoB 2 guidelines consist of extensive and step-by-step set of instructions that utilize signaling questions, which are detailed in subsection~\ref{met:annot_scheme}.
Even though RoB 2 guidelines are widely used for bias assessment, there have been some research on the reliability of these guidelines.
This reliability concern of RoB 2 has been extensively investigated by Minozzi \textit{et al.}. 
They formulated specific instructions on how to approach and answer the signaling questions of RoB 2.
These instructions, referred to as the Instruction Document (ID), address the subjectivity present in the RoB 2 guidelines and provide clear guidance for the assessment process.
Subjectivity in assessment could potentially result in different evaluators coming to disparate conclusions when analyzing the same trial.
Before implementing the ID, the agreement among four expert RoB assessors was zero, but it improved after adopting the ID.
Several other papers explored subjectivity and reliability of the Cochrane RoB 1.0 and 2 tools~\cite{minozzi2022reliability,da2017effect,loef2022interrater,minozzi2020revised}.
%Anjani: Comments from Roger and Martin about the inherent subjectivity
%The signalling questions are broadly factual but leave room for subjective judgements and aim to facilitate judgements about the risk of bias.


Given the significance of maintaining consistency and reliability among annotators, we undertook the development of precise and clear annotation guidelines.
Working closely with our team of experts, we formatted these guidelines into visual placards.
Each placard takes the form of a flowchart and provides instructions for annotating a single signaling question.
These placards address various facets of RoB 2 subjectivity and the actual process of annotation.
For instance, a common reason for low inter-annotator agreement is when annotators correctly address a bias question but annotate evidence from different parts of the full text. 
To tackle this, our placards restrict annotations for a question to a specific part of the text, such as the Methods section, Results section, Flowchart in the Methods section, etc.
Detailed guidelines and these visual placards are available in the Supplementary material, with an example placard shown in the Figure~\ref{fig:placard}.
These placards also tackle another factor contributing to the low inter-annotator agreement.
For example, while some annotators might annotate an entire paragraph as text evidence to answer a question, others might focus on the most informative portion of the text.
To address this, the placards provide clear guidance on whether annotators should annotate a phrase, a sentence, or a combination of sentences.
%
%
%
\subsection{Annotation}
\label{annotation}
%
%
%
%
\begin{figure}[htb]
    \centering
    \includegraphics[width=0.80\columnwidth]{figures/pawls_layout.png}
    \caption{A screenshot of PAWLS interface with an example PDF and RoB annotations.}
    \label{fig:pawls}
\end{figure}
%
%
%
For every SQ, the annotators were guided to use the complete RoB 2 guidance document along with visual placards that were developed.
They followed these instructions meticulously, going through each placard's signaling question one by one.
The provided instructions directed the annotators to read specific sections of full-text RCT that needed annotation.
Their task involved identifying and highlighting relevant text related to answering the signaling question.
%A total of 60 full-text RCTs were annotated using this approach, covering 22 signaling questions across five risk domains defined by RoB 2.
%Out of the 60, nine were doubly-annotated to capture IAA and the rest were singly annotated.

Tagtog~\footnote{\url{https://www.tagtog.net/}}, a commercial text annotation web application, allows for annotating PDF (Portable Document Format) documents, was used for the annotation~\cite{cejuela2014tagtog}.
Out of the 60 documents, 9 were doubly annotated by two experienced annotators (a epidemiology researcher and an assistant professor) to calculate inter-annotator agreement (IAA) over these documents and the rest were singly annotated by the epidemiology researcher.
After double annotation, we performed conflict resolution to address conflicting annotations, which helped us further calibrate the visual placards.
The conflict resolution was followed by annotating 51 additional RCTs.


After the annotation of 9 doubly-annotated RCTs, we had to stop using tagtog for certain reasons and switched to the PAWLS~\footnote{\url{https://pawls.apps.allenai.org/}} annotation tool, which allows users to annotate PDFs for free~\cite{neumann2021pawls}.
We chose to annotate PDFs rather than plain text because RCT PDFs have a visual format that will be lost upon converting to text. 
For example the structure pertaining to sections and subsections, tables, and figures makes the annotation task quicker for the annotators and increases annotation quality.
Post annotation, the feedback was taken from both the annotators details of which could be found in the supplementary material. %TODO Anjani: Prepare the supplementary material 
%
%
%
\subsection{Inter-Annotator Agreement}
\label{subsec:corpus}
%
Cohen's kappa $\kappa$ is the standard annotation reliability measure for many classification annotation tasks, but it is not a relevant measure for token-level annotation tasks like span annotation and entity annotation.
Therefore, instead of $\kappa$, we report the pairwise F1 measure that disregards out-of-the-span tokens (unannotated tokens), which is the ideal measure of annotation reliability for the token-level annotation tasks~\cite{deleger2012building}.
Pairwise F1 calculates the F1 score for each pair of annotators, treating one annotator's labels as the ``true'' labels and the other annotator's labels as the ``predicted'' labels~\cite{brandsen2020creating}.
We interpret F1 measure as shown in the Table~\ref{tab:iaa_interpret}~\cite{landis1977measurement}.


%
%
%
\begin{center}
 \begin{table}[htb]
   \caption{The table details interpretation of pairwise F1-measure and Cohen's Kappa.}\label{tab:iaa_interpret}
 \centering
    \begin{tabular}{lr}
    \toprule
    Agreement interpretation & IAA range \\ 
    \midrule
        Poor & 0-0.99 \\ 
        Slight & 1 - 20.99 \\ 
        Fair & 21 - 40.99 \\ 
        Good & 41 - 60.99 \\ 
        Substantial & 61 - 80.99 \\ 
        Almost perfect & 81 - 99.99 \\ 
        Perfect & 100 \\ 
    \bottomrule
    \end{tabular}
 \end{table}   
\end{center}
%
%
%
%
%
%
\subsection{LLM evaluation}
\label{method:llm}
%
Our annotation guidelines and annotations were adapted for benchmarking classical supervised machine learning approaches and not LLMs.
So even though we were annotating PDFs, we had to restrict a lot of annotations based on the assumption that PDF will be converted into text via OCR (optical character recognition) losing its structure of tables and figures, which anyway a classical ML model could not use without extensive modifications~\cite{li2019figure,li2023uttsr}.
However, recent advancements with LLMs that encode vast amounts of knowledge offers a better alternative made us rethink the evaluation.
The bar for clinical applications is high and it is imperative to evaluate these models for the more challenging clinical tasks like bias span extraction~\cite{singhal2023large}.
Additionally, the tools like ChatPDF~\footnote{\url{https://www.chatpdf.com/}} allows direct interaction between ChatGPT and PDFs, negating the clumsy PDF to text conversion.
Therefore, we consider it essential to evaluate LLMs instead of forcefully adapting the evaluation to a classical machine learning problem.
We formulated the task as a zero-shot RoB extraction task.
This is to gauge whether ChatGPT encodes knowledge related to clinical trial biases.
We will use simple prompt constructs of the structure ``Answer the signalling question + Action item to extract sentence supporting the answer''. Consider the following example.

\begin{quote}
\itshape Example prompt: Question 4.3 Were outcome assessors aware of the intervention received by study participants? Provide an answer and extract the supporting sentences that you write your answer based on. Extract the sentences in JSON~\footnote{JSON = JavaScript Object Notation}.
\end{quote}

The prompt serves two purposes for evaluating LLMs for correctness.
ChatPDF is tasked with 1) answering the signalling question with a response option (the judgment), and 2) extract the text evidence to support your answer.
When answering a question, ChatPDF finds the most relevant paragraphs from the PDF and uses the ChatGPT API from OpenAI to generate an answer.
ChatPDF employs GPT (Generative Pretrained Transformer) 3.5.
Basically ChatGPT via ChatPDF is required to do the same task as human annotators and they will be evaluated on the basis of correctness of the answer.
If LLM answer corresponds to response option selected by the expert annotator, it is considered a correct answer. 
If the text extracted by LLMs as evidence for answering the signaling question fuzzy matches the text selected by the expert annotator, it is considered a correct answer.
Both of these skills will be evaluated using a percentage agreement metric, which is essentially the number of documents for a RoB SQ where ChatGPT's responses align with those of the human expert, divided by the total number of documents assessed.
In several instances, there was no information found by the expert annotator from the RCT to answer a question. 
For such instances, if ChatPDF correctly identifies the absence of relevant information to answer a question, it is considered a correct response.
It's important to highlight that the evaluation framework is designed solely to measure the agreement of the answers between ChatPDF and the expert.
This evaluation was conducted for 25 out of the 60 RCTs and each document was evaluated for X out of the 22 signalling questions. %TODO - replace X with the number of bias questions evaluated
Whenever ChatGPT came to same response judgment but using different text compared to the expert identified one, we sought to confirm with the expert on whether ChatGPT was correct or misinterpreting the text or completely hallucinating.
%
%
%
\section{Results}
\label{sec:results}
%
This section outlines the annotated dataset, presents the inter-annotator agreement findings, and the outcomes of the LLM evaluation on certain RoB signalling questions.
%
%
%
\subsection{The corpus}
\label{subsec:corpus}
%
%Anjani - TODO: Ask Roger for the annotation files from PAWLS and start documenting with whatever annotations are already made.
In this section, we provide key statistical information about the annotations in our corpus.
The histogram in Figure \ref{fig:ann_counts} shows a visual representation of the absolute counts of annotations (tokens) for each of the RoB signaling questions.
SQ 1.3 has disproportionately higher number of annotated tokens, while for all other SQs, the number of annotated tokens remains consistently below 2000 across the entire corpus.

 
There are no token annotations for the bias question 2.4 ``Were these deviations likely to have affected the outcome?'' which hinges on the previous question 2.3.
Table~\ref{table:stats} lists down essential information on the absolute and average annotation lengths for each RoB SQ along with the total number of documents the annotations are identified from.
Annotations from risk domain one (randomization process) are found in almost all the documents, while information related to answering the other risk questions is only available in a small subset of the total annotated RCTs, as also depicted in Figure~\ref{fig:rob_information}.
The figure also shows that for the majority of the RoB questions, there was no information (yellow bar) available both pertaining to answering it and make a judgment as to whether the bias was high or low.
In cases where information was available, the bias tended to be low, as indicated by the prevalence of green bars, except for signaling questions 2.1, 2.2, 3.1, 4.3, 4.4, and 5.2 where bias was high as indicated by the red bars.
%
%
%
\begin{figure}[htb]
    \centering
    \includegraphics[width=0.90\columnwidth]{figures/sq_ann_raw_counts.png}
    \caption{Total number of token annotations for each RoB signalling question.}
    \label{fig:ann_counts}
\end{figure}
%
%
%

%
%
%
\begin{table}[htb]
    \centering
    \caption{General statistics for the annotated corpus}
    \label{table:stats}
    \begin{tabular}{crrrrr}
    \hline
        Signalling question & Total tokens & Average length & Total documents \\ \hline
        RoB 1.1 & 372 & 24.8 & 15 \\ 
        RoB 1.2 & 305 & 19.0625 & 16 \\ 
        RoB 1.3 & 6649 & 415.5625 & 16 \\ 
        RoB 2.1 & 260 & 37.14285714 & 7 \\ 
        RoB 2.2 & 710 & 59.16666667 & 12 \\ 
        RoB 2.3 & 262 & 65.5 & 4 \\ 
        RoB 2.5 & 14 & 14 & 1 \\ 
        RoB 2.6 & 278 & 30.88888889 & 9 \\ 
        RoB 2.7 & 52 & 17.33333333 & 3 \\ 
        RoB 3.1 & 754 & 62.83333333 & 12 \\ 
        RoB 3.2 & 78 & 39 & 2 \\ 
        RoB 3.3 & 56 & 18.66666667 & 3 \\ 
        RoB 3.4 & 140 & 17.5 & 8 \\ 
        RoB 4.1 & 101 & 25.25 & 4 \\ 
        RoB 4.2 & 235 & 29.375 & 8 \\ 
        RoB 4.3 & 135 & 22.5 & 6 \\ 
        RoB 4.4 & 134 & 22.33333333 & 6 \\ 
        RoB 4.5 & 390 & 55.71428571 & 7 \\ 
        RoB 5.1 & 198 & 28.28571429 & 7 \\ 
        RoB 5.2 & 94 & 47 & 2 \\ 
        RoB 5.3 & 287 & 57.4 & 5 \\ \hline
    \end{tabular}
\end{table}
%
%
%

%
%
%
\begin{figure}[htb]
    \centering
    \includegraphics[width=0.90\columnwidth]{figures/judgment_dist.png}
    \caption{Distribution of bias judgment across RoB signalling questions in RoBuster.}
    \label{fig:rob_information}
\end{figure}
%
%
%


%
%
%
\subsection{Inter-annotator agreement}
%
Table~\ref{tab:IAA_sq} illustrates the levels of inter-annotator agreement between the two consensus annotators, both before and after the development of the guidelines.
The IAA scores before and after the guideline improvement are calculated on a different set of documents.
We saw an improvement in the agreement for many signalling questions, but considerable improvement of more than 50 points for the signalling questions number 1.3, 2.1, 4.1 and 4.4.
The IAA between the two reference annotators for a total of 11 out of 22 questions stood to zero before the guideline improvement.
Only for two signalling questions, 4.4 and 5.2, did the guideline improvement raise the IAA from zero to 56.25 and 49.49 respectively.
However, for the signalling questions 2.3 and 2.7, the agreement dropped from a meagre 5.42 and 6.52 to zero.
For the signalling question 1.2, the agreement dropped by 6.28 IAA points making it a drop for three questions.
Inferring from the table~\ref{tab:iaa_interpret} and table~\ref{tab:IAA_sq}, annotators for 11 of the 22 signalling questions had a poor agreement, three of the 22 questions had a fair agreement, four out of the 22 questions had a good agreement and two out of the 22 questions had a substantial agreement while only two of the 22 questions had an almost perfect agreement of beyond 81 IAA points.
As expected, none of the SQ annotations had a perfect IAA of 100.
%
%
%
\begin{table}[htb]
    \caption{IAA before and after the visual placards development}
    \label{tab:IAA_sq}
    \centering
    \begin{tabular}{crrr}
    \hline
        Signalling question & before guideline improvement & after guidelines improvement & change \\
    \hline
        RoB 1\_1 & 24.44 & 55.02 & +30.58 \\ 
        RoB 1\_2 & 50.28 & 44 & -6.28 \\ 
        RoB 1\_3 & 20.44 & 90.9 & +70.46 \\ 
        RoB 2\_1 & 1.34 & 67.26 & +65.92 \\ 
        RoB 2\_2 & 7.23 & 38.66 & +31.43 \\ 
        RoB 2\_3 & 5.42 & 0 & -5.42 \\ 
        RoB 2\_4 & - & 0 & 0 \\ 
        RoB 2\_5 & 0 & 0 & 0 \\ 
        RoB 2\_6 & 68.85 & 83.25 & +14.4 \\ 
        RoB 2\_7 & 6.52 & 0 & -6.52 \\ 
        RoB 3\_1 & 23.57 & 39.68 & +16.11 \\ 
        RoB 3\_2 & 0 & 0 & 0 \\ 
        RoB 3\_3 & 0 & 0 & 0 \\ 
        RoB 3\_4 & 0 & 0 & 0 \\ 
        RoB 4\_1 & 6.51 & 61.71 & +55.2 \\ 
        RoB 4\_2 & 0 & 0 & 0 \\ 
        RoB 4\_3 & 13.85 & 30.21 & +16.36 \\ 
        RoB 4\_4 & 0 & 56.25 & +56.25 \\ 
        RoB 4\_5 & 0 & 0 & 0 \\ 
        RoB 5\_1 & 0 & 0 & 0 \\ 
        RoB 5\_2 & 0 & 49.49 & +49.49 \\ 
        RoB 5\_3 & 0 & 0 & 0 \\ \hline
    \end{tabular}
\end{table}
%
%
%
\subsection{LLM evaluation}
%
In the Table~\ref{table:LLM_eval}, we report the accuracy of simple prompts as described in section~\ref{method:llm}.

For the first risk domain, ChatGPT has high agreement for the SQ 1.1, but low agreement for 1.3.
The agreement considerably drops for the risk domain number two.
%
%
%
\begin{table}[htb]
    \caption{LLM accuracy}
    \label{table:LLM_eval}
    \centering
    \begin{tabular}{crr}
    \hline
     & \multicolumn{2}{c}{Simple prompts} \\
        Signalling question & Accuracy (response option) & Accuracy (evidence extraction) \\ 
    \hline
        RoB 1\_1 & 88\% & 72\% \\ 
        RoB 1\_2 & 69.56\% & 58.33\%  \\ 
        RoB 1\_3 & 40\% & 40\% \\ 
        RoB 2\_1 & &  \\ 
        RoB 2\_2 & &  \\ 
        RoB 2\_3 & &  \\ 
        RoB 2\_4 & &  \\ 
        RoB 2\_5 & &  \\ 
        RoB 2\_6 & &  \\ 
        RoB 2\_7 & &  \\ 
        RoB 3\_1 & &  \\ 
        RoB 3\_2 & &  \\ 
        RoB 3\_3 & &  \\ 
        RoB 3\_4 & &  \\ 
        RoB 4\_1 & & \\ 
        RoB 4\_2 & & \\ 
        RoB 4\_3 & &  \\ 
        RoB 4\_4 & &  \\ 
        RoB 4\_5 & &  \\ 
        RoB 5\_1 & &  \\ 
        RoB 5\_2 & &  \\ 
        RoB 5\_3 & &  \\ \hline
    \end{tabular}
\end{table}
%
%
%
\section{Discussion}
\label{sec:discussion}
%
%
%
% Why do some RoB questions have more annotations compared to the other? Does it mean papers have more information on certain aspects compared to the other questions?
The immediate point we noticed is that SQ 1.3 has disproportionately higher number of annotated tokens because it involved annotating full tables for coming to a bias judgement.

% Were the visual placards effective?
% Why were agreements on certain questions zero?
IAA agreement was calculated before and after visual placard development.
Using the visual placards improves the agreement for choosing the RCT text evidence for 10 of the 22 SQs showing their utility in aiding decision making.
However, we also notice that some questions had zero agreement.
After the annotation process, we recorded the annotators opinion on how difficult was it for them to annotate text evidence for answering each signalling question.
% We had employed a form to gauge how difficult each RoB question was to answer and annotate.
The form collected opinions of the annotators on how difficult each RoB question was for annotation on a scale of 1 to 5, with 1 being very easy and 5 being very difficult.
These results correlate with the opinion from the form filling.
Almost all the RoB questions where the IAA between the annotators was zero (2.3, 2.4, 2.5, 2.7, 3.3, 3.4, 4.5) were marked with ``difficult'' to annotate.
The poor agreement is caused by the subjectivity of the signalling questions, lack of information to annotate in the clinical trial, theoretical nature of the signalling questions and the complex process of analysing trials rather than it being about the expertise of the annotators.
The annotators / reviewers have years of experience writing SRs and carrying out RoB assessment.

% Why are these questions difficult to answer? - subjective judgment, lack of information to annotate, unclear information or ambiguous information?
% Were certain RoB questions more subjective than the others?

% Theoretical nature of the signalling questions?
Consider the questions 3.3 and 3.4, and 4.4 and 4.5.

% Why did agreement increase for certain questions?
It was for the signalling question 1.3 that the agreement increased the most. 
The reason is because the visual placards restrict looking for answering this question only from the patient characteristics table from the RCT leading to both annotators consistently annotating only tables for this SQ and raising the IAA in the process.
The agreement increased for the SQ 1.1 as well in which the visual placards limited to annotating the information about the randomization methodology employed in the trial and limiting it to annotating in a particular sections of the RCT.
These improvements were also the reason for an increased agreement for the SQs 2.1 and 2.2.



% Was the bias in subjective outcomes higher than in the objective outcomes?
OXO






\begin{enumerate}
    \item Was information not available to answer certain RoB SQs?
    \item Were some questions more theoretical in comparison to others? How did they impact the text annotation? For example, wherever there was no explicit information, it was required to annotate the ``outcome'' descriptions because the judgment of a particular SQ could be dependent on the biases that a particular outcome could cause. For example, it is not possible to blind the intervention administrators for certain interventions in physiotherapy and rehabilitation. therefore, wherever the reviewers found such an intervention, they had to use this information to mark the judgment.
    \item Discuss the kind of conflict identified in the doubly-annotated documents.
    \item How were these conflicts resolved?
    \item Discuss the improvements suggested by Julian Higgins. For example, his hesitations with the thresholds used to simplify the judgment.
    \item Explain how some RoB SQ assessments needed to be made simpler to ensure translation to text annotation.
\end{enumerate}

%
%
%
\subsection{Conflict resolution}
%
Our aim was to use the annotated corpus and resolve the conflicts between the annotators to increase the annotation agreement and also improve the instructions in the placards.
For multiple conflicting cases, the instructions were either unclear, interpreted differently, or the annotators forgot to annotate.
However, when the annotators went through the conflicts, they were able to resolve them.


During conflict resolution, we found that certain instances where both annotators marked the same text evidence to answer the same SQ but gave different .
We resorted to face-to-face conflict resolution in this case for every conflict case and updated the visual placards.
We went for conflict resolution in the scenarios when
1) the annotators marked different parts of text to answer the same question with same judgment. (\url{https://docs.google.com/spreadsheets/d/14jYqXTcnVzZsC5pbKQ2GcwtVrwl81DG9Yo5uhCJZzjo/edit#gid=1107412538})
2) the annotators marked same part of text to answer the same question, but gave different judgment option for it. (\url{https://docs.google.com/spreadsheets/d/14jYqXTcnVzZsC5pbKQ2GcwtVrwl81DG9Yo5uhCJZzjo/edit#gid=0})
Both annotators use different parts of text to answer the same signalling question, but give different judgment options. \url{https://docs.google.com/spreadsheets/d/14jYqXTcnVzZsC5pbKQ2GcwtVrwl81DG9Yo5uhCJZzjo/edit#gid=1107412538}
In case, one annotator selected a part of text and another the same text but an additional text too, then we will select the annotations from the longer annotation.
%
%
%
\subsection{LLM evaluation}
\label{disc:llm}
%
%TODO: Discuss how the prompt was set to only extract the sentences from the uploaded PDF.
In our initial experiments, we had a few examples where ChatPDF generated an answer to the signalling question, but these sentences comprising the answer were not to be found in the input PDF document.
In one particular instance, LLM answered correctly saying that the sequence was randomly generated and also extracted the sentence related to answering the question, but the generated sentence was not found in the PDF.
This is likely a case of hallucination.
In such cases, the PDF was reloaded into ChatPDF and the re-prompted for that particular signalling question.
In cases where no information was found to answer a SQ, ChatPDF generated a response by itself giving example sentences that could be used to answer that question.
We had to 
%Can retrieve multiple related answers for a RoB question prompt and one of that has to be the correct answer and should be the same as what the annotators annotated.
%After instructing ChatPDF to look into the uploaded PDF and not generate sentences by itself, it gives the following output.
%``I apologize, but as an AI language model, I do not have access to any uploaded PDFs or any external sources. Please provide me with the PDF you are referring to, and I will be happy to extract the sentences related to answering the signalling question RoB 1.1.''


% Risk Domain one
For the first risk domain, we saw a low agreement between ChatGPT and the experts because the answer to this SQ is typically found in the table text of RCT.
ChatPDF can not yet read images in the PDF, including images that consist of scanned text. Text in tables is read by ChatPDF, but it might have problems correlating the correct rows and columns.
Maybe this was the reason why ChatPDF sought to answer this question using RCT text rather than the table text.

% Risk Domain two
OXO
%
%
%
\subsection{Limitations}
\label{subsec:limits}
%
Our study has the following limitations.
The first limitation arises from the relatively small scale of our annotations, with only 60 documents undergoing the annotation process.
This constraint is primarily due to financial constraints, as the availability of funds limited our ability to hire a larger number of expert annotators.
Despite the limitation, the robustness of the annotations provided by the experts remains noteworthy, as they thoroughly assessed the selected RCTs within the resources at hand.
A second limitation stems from the narrow focus of annotations, concentrated exclusively on physiotherapy and rehabilitation clinical trials.
This specificity arises from our reliance on domain experts for annotations, who possess the requisite expertise in these areas.
While this targeted approach guarantees high-quality annotations within the specified domains, it could inadvertently restrict the broader applicability of the annotated corpus.
%Thirdly, our study's scope is limited by the selection of particular journals to construct the corpus in addition to certain RCTs selected randomly.
%This choice was necessitated by the desire to incorporate articles of the utmost quality, ensuring the inclusion of high-caliber research in the analysis.
%While this decision ensures a certain standard of literature, it inadvertently excludes a wider spectrum of articles, potentially biasing the corpus toward a specific subset of clinical trials.
%It's important to note that the chosen journals might have their own editorial policies, thereby limiting the diversity of included studies.
%This methodological trade-off, while benefiting the corpus's overall quality, might inadvertently influence the representation of risk of bias annotations, particularly in relation to lower quality trials.
Our LLM evaluation was limited by the fact that we chose to work with PDFs. 
There are limited platforms that interact with PDFs and this restricts our choice of the models to evaluate.
Google's Bard does interact with PDFs, but we observed that the Bard results were less deterministic than ChatPDF.
%
%
%
\section{Conclusion}
\label{sec:conclusion}
%
We have presented RoBuster: a new, publicly-available corpus, comprising 60 full-text RCTs richly annotated with RoB span information for 22 risk of bias questions.
The dataset fills a need for a corpus to evaluate risk of bias span extraction using machine learning technologies.
Our corpus has detailed and fine-grained information in form of individual risk of bias spans along with annotator decision on the bias risk (high or low).
We used RoBuster as a benchmark to evaluate LLMs for how well LLMs agree with human-led bias assessment.
As it has been developed by bias assessors and natural language processing experts, RoBuster can support automated approaches to bias assessment and can also be a part SLR (living systematic review) systems.
Throughout this work, we've encountered several challenges during RoB annotation.
These experiences have led us to outline refinements in the discussion section, offering insights to guide future studies in this domain.
In the future, we plan to extend this corpus by adding more annotated RCT full-texts.
%In addition, LLMs could be fine-tuned with the available annotations to further refine the corpus with new annotations which could have been missed.
We also plan to test the visual annotation placards to train trainee risk of bias assessors.
%
%
%
\section{Abbreviations}%% if any
%
\begin{enumerate}
    \item RCT - Randomized Controlled Trial
    \item RoB - Risk of Bias
    \item SR - Systematic review
    \item SQ - Signalling Question
    \item SLR - Living Systematic Review
    \item LLM - Large Language Model
    \item GPT - Generative Pre-trained Transformer
    \item NLP - Natural Language Processing
    \item IAA - Inter-annotator Agreement
    \item PEDro - Physiotherapy Evidence Database RoB scale
    \item AMSTAR - A MeaSurement Tool to Assess systematic Reviews - Risk of Bias
    \item EPOC RoB - Effective Practice and Organisation of Care - Risk of Bias tool
    \item CDSR - Cochrane Database of Systematic Reviews
    \item PDF - Portable Document Format
    \item JSON - JavaScript Object Notation
\end{enumerate}
%
%
%
\backmatter

\bmhead{Supplementary information}
%
The visual placards along with detailed annotation instructions, the RCTs used in this study along with their license information, and an extended discussion section are included in the supplementary information files.
%
%
%
\bmhead{Acknowledgments}

%Acknowledgements are not compulsory. Where included, they should be brief. Grant or contribution numbers may be acknowledged.

%Please refer to Journal-level guidance for any specific requirements.
We thank Dr. Julian Higgins for detailed comments and improvements on our visual annotation placards that helped us further refine them post conflict resolution. 
%
%
%
\section*{Declarations}
%
\subsection*{Funding}
%
This research was supported by HES-SO, Valais-Wallis, Sierre, Switzerland. 
%
%
%
\subsection*{Conflict of interest}
%
The authors declare that they have no competing interests.
%
%
%
\subsection*{Competing interest}
%
The authors declare that they have no competing interests.
%
%
%
%\subsection*{Ethics approval}
%
%Not applicable
%
%
%
\subsection*{Consent to participate and publish}
%
The experts who undertook the visual placards development and the annotation process for this corpus were explained the purpose of the annotation project and agreed to voluntarily participate in the study.
Even though they agreed to participate in the study, they can withdraw their participation any time without consequences of any kind.
They were explained the purpose and nature of the study in form of a presentation and had an opportunity to ask questions.
They were also fully informed about the purpose of the study and the eventual publication of the findings.
Each expert provided explicit consent for the publication of the annotated corpus, along with the understanding that any identifying information would be appropriately anonymized to protect their privacy. 
%
%
%
\subsection*{Data availability}
%
Availability of data and materials on the journals preferred portal.
%
%
%
\subsection*{Code availability}
%
The notebooks used analyze the corpus will be available on GitHub.
%
%
%
%\subsection*{Authors' contributions}
%
%Authors' contributions
%BMC journals: \url{https://www.biomedcentral.com/getpublished/editorial-policies}
%
%
%
%%===================================================%%
%% For presentation purpose, we have included        %%
%% \bigskip command. please ignore this.             %%
%%===================================================%%
%
\begin{appendices}
%
%
%
\section{Annotation guidelines}\label{annot_guidelines}
%
An appendix contains supplementary information that is not an essential part of the text itself but which may be helpful in providing a more comprehensive understanding of the research problem or it is information that is too cumbersome to be included in the body of the paper.
%
%
%
\end{appendices}

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

\bibliography{bibliography.bib}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl


\end{document}
